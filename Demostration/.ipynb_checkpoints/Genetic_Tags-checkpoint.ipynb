{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
    "                                  weights=w).ravel()\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y\n",
    "\n",
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "X, Y = nudge_dataset(X, digits.target)\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "X[X>0.5] = 1.0\n",
    "X[X<=0.5] = 0.0\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Models we will use\n",
    "logistic = linear_model.LogisticRegression()\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f450647ae90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "print( X_train[0].shape)\n",
    "plt.imshow(X_train[0].reshape(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.64, time = 0.11s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -24.36, time = 0.09s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -23.13, time = 0.08s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -22.66, time = 0.07s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -22.45, time = 0.08s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -22.00, time = 0.07s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -21.87, time = 0.07s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -21.65, time = 0.08s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -21.55, time = 0.07s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -21.51, time = 0.08s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -21.46, time = 0.08s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -21.34, time = 0.08s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -21.79, time = 0.11s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -21.25, time = 0.10s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -21.28, time = 0.08s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -21.21, time = 0.09s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -21.45, time = 0.11s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -21.37, time = 0.10s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -21.16, time = 0.10s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -21.16, time = 0.09s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('rbm', BernoulliRBM(batch_size=10, learning_rate=0.06, n_components=10, n_iter=20,\n",
       "       random_state=0, verbose=True)), ('logistic', LogisticRegression(C=6000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 10\n",
    "logistic.C = 6000.0\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "classifier.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.53      0.53       716\n",
      "          1       0.47      0.38      0.42       726\n",
      "          2       0.49      0.52      0.50       719\n",
      "          3       0.40      0.33      0.36       721\n",
      "          4       0.58      0.64      0.61       719\n",
      "          5       0.35      0.48      0.41       729\n",
      "          6       0.60      0.73      0.66       698\n",
      "          7       0.47      0.70      0.56       741\n",
      "          8       0.51      0.08      0.14       688\n",
      "          9       0.42      0.39      0.40       731\n",
      "\n",
      "avg / total       0.48      0.48      0.46      7188\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        Y_train,\n",
    "        classifier.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.2, 4))\n",
    "for i, comp in enumerate(rbm.components_):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.suptitle('100 components extracted by RBM', fontsize=16)\n",
    "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
